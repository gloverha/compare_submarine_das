{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reprocessing Nov 2021 DAS results using empirical correction coefficients\n",
    "- up to date for CODAS data as of August 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from scipy import signal\n",
    "\n",
    "from datetime import date, timedelta, datetime\n",
    "import matplotlib.dates as mdates\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "pltdir = '/Users/msmith/Documents/DAS/2021Test/'\n",
    "\n",
    "import scipy.io\n",
    "import warnings\n",
    "warnings.filterwarnings('once')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to reprocess using frequency-dependent conversion factor for each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rewritten wave conversion to use strain_factor as function of frequency...\n",
    "# april 5, 2023: now includes attenuation, as this is not accounted for in empirical correction factor \n",
    "\n",
    "def DAS_wave_conversion_f(das_data,f_cutoff,E_corr_factor,freq,depth):\n",
    "    #function to use simple pwelch to estimate wave spectra and bulk wave parameters for DAS data\n",
    "    #assume some arbitrary strain factor that results in decent looking results, for now...\n",
    "    \n",
    "    #E_corr_factor is frequency dependent correction factor to apply to SPECTRA - no depth correction needed??\n",
    "    \n",
    "    #pwelch - defualt is 50% overlap\n",
    "    window = 128\n",
    "    nfft = 256\n",
    "    f_psd, ds_psd = signal.welch((das_data.data),fs=2,nfft=nfft,nperseg=window)#,window=[128]\n",
    "    \n",
    "\n",
    "    #depth attenuation correction (using depth from netcdf)\n",
    "    #depth = -das_data.bathy_m\n",
    "    \n",
    "    k = (2*np.pi*freq)**2 / 9.8\n",
    "    attenuation = np.exp(k*depth)\n",
    "    attenuation = attenuation**2; # square for energy \n",
    "\n",
    "    #interpolate onto SWIFT frequencies, then multiply by empirical correction factor\n",
    "    from scipy.interpolate import interp1d\n",
    "    ds_psd = interp1d(f_psd, ds_psd)(freq)\n",
    "    ds_psd_corr = (ds_psd*E_corr_factor/attenuation).values\n",
    "    \n",
    "    \"\"\"\n",
    "    f_cutoff_i = [i for i, x in enumerate(ds_DAS.frequency > f_cutoff) if x][0] #first value greater than defined cutoff\n",
    "    f_eqrange = ds_DAS.frequency[f_cutoff_i::]\n",
    "    spec_new = ds_psd_corr\n",
    "    spec_new[f_cutoff_i::]= ds_psd_corr[f_cutoff_i]/(f_eqrange[0]**(-4))*(f_eqrange**(-4))\n",
    "    ds_psd_corr = spec_new\n",
    "    #plt.loglog(ds_DAS.frequency,spec_new,linestyle='dotted',color=colors_spectra[ti])\n",
    "    \n",
    "    ## NEW METHOD FOR ROLLOFF OF SPECTRA\n",
    "    spec_sel = ds_psd_corr\n",
    "    max_i = argrelextrema(np.array(spec_sel), np.less)\n",
    "    peak_minfreq = 0.15\n",
    "    peak_i = max_i[0][[i for i, x in enumerate(freq[max_i]>peak_minfreq) if x]][0]\n",
    "    \n",
    "    \n",
    "    noise_floor = 350\n",
    "    floor_i = np.array([i for i, x in enumerate(ds_psd < noise_floor) if x] )\n",
    "    peak_i = floor_i[floor_i > 17][0]\n",
    "    \n",
    "    \n",
    "    f_eqrange = ds_DAS.frequency[peak_i::]\n",
    "    spec_new = ds_psd_corr\n",
    "    spec_new[peak_i::]= ds_psd_corr[peak_i]/(f_eqrange[0]**(-4))*(f_eqrange**(-4))\n",
    "    ds_psd_corr = spec_new\n",
    "    #plt.loglog(ds_DAS.frequency,spec_new,linestyle='dotted',color=colors_spectra[ti])\n",
    "    ##\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    plt.loglog(freq,ds_psd_corr)\n",
    "    plt.xlabel('Hz');plt.ylabel('E');plt.grid(True,which='minor');plt.xlim(left=.09)\n",
    "    plt.show(block=False)\n",
    "    plt.pause(1)\n",
    "    plt.close()\n",
    "    \"\"\"\n",
    "    \n",
    "    #calculate bulk wave characteristics\n",
    "    max_i = np.argmax(ds_psd_corr)\n",
    "    Tp = 1/(f_psd[max_i])\n",
    "    \n",
    "    psd_fwaves = ((freq > 0.03) & (freq < .5))\n",
    "    fe = ((ds_psd_corr[psd_fwaves] * freq[psd_fwaves]) /ds_psd_corr[psd_fwaves].sum() ).sum() #(f*E)/E\n",
    "    #fe = ((ds_psd_corr[psd_fwaves] * f_psd[psd_fwaves]) /ds_psd_corr[psd_fwaves].sum() ).sum() #(f*E)/E\n",
    "    Te = 1/fe\n",
    "    \n",
    "    bandwidth = (freq[1::].values - freq[0:-1].values).mean()\n",
    "    Hs = 4*np.sqrt( ds_psd_corr[psd_fwaves].sum() * bandwidth ) \n",
    "    \n",
    "    return f_psd, ds_psd, ds_psd_corr, Tp, Te, Hs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CODAS_waves_to_netcdf_f(das_directory, output_directory,f_cutoff_file):\n",
    "    #using frequency and channel dependent E_corr_factor from August 2022 calibration \n",
    "\n",
    "    #get all channel folders for this period\n",
    "    das_chanfolders = glob.glob(das_directory + '*')\n",
    "    das_channels = [round(float(x.split('/')[-1])) for x in das_chanfolders]\n",
    "    das_channels.sort()\n",
    "\n",
    "    #get all times \n",
    "    #das_timestr = sorted([x[60:75] for x in glob.glob(das_chanfolders[0]+'/*.ncdf')])\n",
    "    das_times = sorted([datetime.strptime(x[60:75],'%Y%m%d_%H%M%S') for x in glob.glob(das_chanfolders[0]+'/*.ncdf')])\n",
    "\n",
    "    f_cutoff = 1\n",
    "\n",
    "    #create stacked arrays of outputs for all times, channels\n",
    "    das_Hs = []\n",
    "    das_Tp = []\n",
    "    das_Te = []\n",
    "    das_psd = []\n",
    "    das_psd_corr = []\n",
    "    for di,das_channel in enumerate(das_channels[393:394]): #393:495 CHECK THIS IS CORRECT!!, match in inputs to xarray\n",
    "        \n",
    "        das_info = pd.read_csv('CODAS_info.csv')\n",
    "        das_info_channel = das_info[das_info['Channel']==das_channel]\n",
    "        depth = -das_info_channel['Water Depth'].values\n",
    "    \n",
    "        #correction coefficient files \n",
    "        \"\"\"coeff_dir = '/Users/msmith/Documents/DAS/CODAS/DASresults_JT_0324/'\n",
    "        mat = scipy.io.loadmat(coeff_dir+'DASspecta_channel'+str(das_channel)+'.mat')\n",
    "        mat['E_ratio'][22,:] = np.nan\n",
    "        mat['f'] = np.concatenate( mat['f'], axis=0 )\n",
    "        E_ratio_f = np.nanmedian(mat['E_ratio'],0) #average ratio as a function of frequency\n",
    "        \"\"\"\n",
    "        coeffdir = '/Users/msmith/Documents/DAS/CODAS/202208_reprocessing/'\n",
    "        correction_channel = pd.read_csv(coeffdir+'SpectralCorrectionFactor_channel'+str(das_channel)+'_1Hz.csv')\n",
    "        E_ratio_f = correction_channel['corr_factor']\n",
    "        freq = SWIFT.freq#correction_channel['freq']\n",
    "\n",
    "        Hs_inner = []\n",
    "        Tp_inner = []\n",
    "        Te_inner = []\n",
    "\n",
    "        psd_inner = []\n",
    "        psd_corr_inner = []\n",
    "        for ti,das_time in enumerate(das_times):\n",
    "            das_timestr = datetime.strftime(das_time,format='%Y%m%d_%H%M%S')\n",
    "            das_file = glob.glob(das_directory + str(das_channel) + '/' + 'CODAS.D*__' + das_timestr+'.*__chn*'+str(das_channel)+'.ncdf')[0]\n",
    "            ds_disk = xr.open_dataset(das_file)\n",
    "            \n",
    "            f_cutoff = f_cutoff_file[ti][0]\n",
    "            f_psd, ds_psd, ds_psd_corr, Tp_psd, Te_psd, Hs_psd = DAS_wave_conversion_f(ds_disk,f_cutoff,E_ratio_f,freq,depth)\n",
    "            Hs_inner.append(Hs_psd)\n",
    "            Tp_inner.append(Tp_psd)\n",
    "            Te_inner.append(Te_psd)\n",
    "\n",
    "            psd_inner.append(ds_psd)\n",
    "            psd_corr_inner.append(ds_psd_corr)\n",
    "        das_Hs.append(Hs_inner)\n",
    "        das_Tp.append(Tp_inner)\n",
    "        das_Te.append(Te_inner)\n",
    "\n",
    "        das_psd.append(psd_inner)\n",
    "        das_psd_corr.append(psd_corr_inner)\n",
    "\n",
    "\n",
    "\n",
    "    #consolidate into xarray\n",
    "\n",
    "    # define data with variable attributes\n",
    "    data_Hs = {'Hs':(['channels','time'], das_Hs, \n",
    "                             {'units': 'm', \n",
    "                              'long_name':'significant wave height'})}\n",
    "    data_Tp = {'Tp':(['channels','time'], das_Tp, \n",
    "                             {'units': 's', \n",
    "                              'long_name':'peak wave period'})}\n",
    "    data_Te = {'Te':(['channels','time'], das_Te, \n",
    "                             {'units': 's', \n",
    "                              'long_name':'energy-weighted wave period'})}\n",
    "\n",
    "    data_E = {'E':(['channels','time','frequency'], das_psd, \n",
    "                             {'units': 'm', \n",
    "                              'long_name':'energy spectrum'})}\n",
    "    data_E_corr = {'E_corr':(['channels','time','frequency'], das_psd_corr, \n",
    "                             {'units': 'm', \n",
    "                              'long_name':'corrected energy spectrum'})}\n",
    "\n",
    "    # define coordinates\n",
    "    coords = {'time': (['time'], das_times),\n",
    "              'channels': (['channels'], das_channels[393:394]),\n",
    "             'frequency': (['frequency'], freq)}\n",
    "\n",
    "    # define global attributes\n",
    "    attrs = {'creation_date':datetime.now(), \n",
    "             'author':'M Smith', \n",
    "             'email':'madisonmsmith@whoi.edu'}\n",
    "\n",
    "    # create dataset\n",
    "    ds_Hs = xr.Dataset(data_vars=data_Hs, \n",
    "                    coords=coords, \n",
    "                    attrs=attrs)\n",
    "    ds_Tp = xr.Dataset(data_vars=data_Tp, \n",
    "                    coords=coords, \n",
    "                    attrs=attrs)\n",
    "    ds_Te = xr.Dataset(data_vars=data_Te, \n",
    "                    coords=coords, \n",
    "                    attrs=attrs)\n",
    "\n",
    "    ds_E = xr.Dataset(data_vars=data_E, \n",
    "                    coords=coords, \n",
    "                    attrs=attrs)\n",
    "    ds_Ecorr = xr.Dataset(data_vars=data_E_corr, \n",
    "                    coords=coords, \n",
    "                    attrs=attrs)\n",
    "\n",
    "    #merge datasets of each variable\n",
    "    ds_DAS = xr.merge((ds_Hs,ds_Tp,ds_Te,ds_E,ds_Ecorr))\n",
    "\n",
    "\n",
    "    ds_DAS.to_netcdf(output_directory +'/uw_'+datetime.strftime(das_times[0],'%Y-%m')+'_waveoutputs_23AprCorrFactors_1Hz_v4.nc')\n",
    "    \n",
    "    return ds_DAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/msmith/miniconda3/envs/CMIP6-201910/lib/python3.7/site-packages/ipykernel_launcher.py:68: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "#INPUTS\n",
    "das_directory = '/Users/msmith/Documents/DAS/CODAS/uw_2022_08/'\n",
    "output_directory = '/Users/msmith/Documents/DAS/CODAS'\n",
    "\n",
    "f_cutoff_file = pd.read_csv('202208_freqcutoff_manualCopy2.txt',header=None)\n",
    "ds_DAS = CODAS_waves_to_netcdf_f(das_directory,output_directory,f_cutoff_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
